\section{User Guide}
This project is built completely upon OpenStack cloud computing platform, in which context this documentation shall proceed to be interpreted. The targeted operating system (on servers) is Unix-like operating system, in which the project is tested. In other operating systems (like Mac, Windows), commands might not work as expected. On the other hand, the control operating system (on local node) must be either Unix-like or macOS, as required by \href{https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html}{Ansible}. In this project, we rely on Ansible for all our configuration management and application-deployment tasks.  

Start by cloning the repository \href{https://github.com/nxv1987/UniMelb---Cluster-and-Cloud-Computing-COMP90024-2020-SM1}{Cluster and Cloud Computing 2020} which contains the source code of the project. After cloning, set working directory to the project directory named 'city\_analytics'.  

\subsection{Requirements}
The deployment of the system requires these programs to be installed on the local machine: \href{https://www.python.org/}{Python}, \href{https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html}{Ansible}, \href{https://docs.openstack.org/newton/user-guide/common/cli-install-openstack-command-line-clients.html}{OpenStack Client}, \href{https://docs.openstack.org/nova/latest/#installation}{OpenStack Compute (nova)}, \href{https://docs.openstack.org/openstacksdk/latest/user/}{OpenStack SDK}.  

\textit{Note: Installing \href{https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html}{Ansible} with \href{https://pip.pypa.io/en/stable/}{pip} may be simpler and thus the more preferable way than some other methods.}  

\subsection{Provide credentials for connection}
\textit{Note: if it is the first time you are granted access to the cloud service or you did not register for the service with
a password, you may have to reset your password for valid authentication.}  

One way to input credentials is by running a bash script to set environment variables.  
\begin{lstlisting}[language=bash]
source credentials/openrc.sh
\end{lstlisting}

Alternatively, 'credentials/clouds\_template.yaml' can be used. See the documentation inside file for details.  

\subsection{Set up servers}
Run the following command to invoke a series of orchestrations that will create and configure cloud instances.  
\begin{lstlisting}[language=bash]
./setup.sh
\end{lstlisting}

The series of orchestrations (described in playbooks) include the following:  
\begin{itemize}
  \item Enable dynamic inventory  
  \item Create security groups  
  \item Generate keypairs  
  \item Launch instances  
  \item Create volumes  
  \item Attach volumes  
  \item Generate inventory file with IP addresses  
  \item Add proxy
  \item Install dependencies
  \item Format external volume
  \item Install docker
  \item Copy project repo to cloud
\end{itemize}

Below are detailed description and invocation of each individual task.  

\subsubsection{Enable the use of dynamic inventory}
Dynamic inventory is available with the use of the script 'inventories/dynamic.py'. For a comprehensive demonstration,
see \href{https://docs.ansible.com/ansible/latest/user_guide/intro_dynamic_inventory.html#inventory-script-example-openstack}{dynamic inventory example}.  

To make the script executable:  
\begin{lstlisting}[language=bash]
chmod +x inventories/dynamic.py
\end{lstlisting}

To test function of the script:  
\begin{lstlisting}[language=bash]
inventories/dynamic.py --list
\end{lstlisting}

\subsubsection{Create security groups}
The parameters set for security groups are defined in 'vars/security.yaml'.  
\begin{lstlisting}[language=bash]
ansible-playbook security_groups.yaml
\end{lstlisting}

\subsubsection{Generate keypairs}
The parameters set for the keypairs are defined in 'vars/access.yaml'.  
\begin{lstlisting}[language=bash]
ansible-playbook keypairs.yaml
\end{lstlisting}
For each keypair generated, the public key is stored in the cloud and the private key locally with access permission changed to 600. In our case, a private key has been placed at 'credentials/common.pem'.  

\subsubsection{Launch compute instances}
Instances are created using the public key stored in the cloud that has the same name as the private key, then added to the appointed security groups.  
\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts.yaml instances.yaml
\end{lstlisting}

The common parameters shared between instances are defined in 'inventories/group\_vars/webservers.yaml'.  

\subsubsection{Create volumes}
The parameters set for volumes are defined in 'inventories/group\_vars/webservers.yaml'. The name of each volume is defined as the name of an instance it is meant for prefixed with 'vol'. This can be changed in the parameter file.  
\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts.yaml volumes.yaml
\end{lstlisting}

\subsubsection{Attach volumes to instances}
The volumes and instances are attached based on the matching of their names.  
\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts.yaml volumes_attach.yaml
\end{lstlisting}

\subsubsection{Export inventory file with IP addresses}
Since instances were created with dynamic IP addresses, it is necessary to obtain those IP addresses for later use
(e.g establishing SSH connection).  
\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/dynamic.py -i inventories/hosts.yaml
inventory_export.yaml
\end{lstlisting}
New inventory file is established at 'inventories/hosts\_auto.ini'.  

To test auto-generated IP addresses, try pinging instances for response:  
\begin{lstlisting}[language=bash]
ansible -i inventories/hosts_auto.ini all -m ping
\end{lstlisting}

\subsubsection{Proxies configuration}
As the instance is behind the University network, instances will need to add proxy settings to access the internet. Append block of proxy settings into /etc/environment for instances. Same needed for docker service, http-proxy.conf.j2 is copied into to /etc/systemd/system/docker.service.d. Daemon and docker are restarted to have new settings to take effect.

\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts_auto.ini proxy_config.yaml 
\end{lstlisting}

\subsubsection{Dependencies for instances}
The following dependencies are installed on servers: 
apt-transport-https, build-essential, ca-certificates, curl, python-dev, git, python-pip, python-setuptools, software-properties-common, unzip and vim

\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts_auto.ini dependencies.yaml
\end{lstlisting}

\subsubsection{Format External /dev/vdb Volume}
This command install XFS file system on the attached volume /dev/vdb. To avoid reinstalling file system again, install xfs and make file system tasks are tagged with format. Using --skip-tags on the command skip these tasks. The formatted volume is mounted on /external. 

\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts_auto.ini formatvolume.yaml
--skip-tags "format"
\end{lstlisting}

\subsubsection{Docker Installation}
This playbook uninstall old docker, new docker installer is obtained from \href{https://download.docker.com/linux/ubuntu/gpg}{docker installer}). Docker and Docker compose is installed after using apt and pip.

\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts_auto.ini docker.yaml
\end{lstlisting}

\subsubsection{Upload Project Folder to Cloud Instances}
Execute following to copy city\_analytics directory to cloud's external storage, if already exist, files will be overwritten. Prior that, /external permission is changed to allow read and write remotely. 

\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts_auto.ini sync_repo.yaml
\end{lstlisting}

\subsubsection{Miscellaneous commands}
To delete instances:  

\textit{\textbf{Warning: this command should be used with high caution as it will delete all instances currently available in connected cloud projet.}}  
\begin{lstlisting}[language=bash]
nova list | awk '$2 && $2 != "ID" {print $2}' | xargs -n1 nova delete
\end{lstlisting}
Source: \href{https://maestropandy.wordpress.com/2016/08/24/openstack-delete-bulk-instances/}{Openstack - Delete Bulk Instances}  


\subsection{Setup Application}
Following shell script invokes the ansible playbook commands to launch each applications on the instances. 

\begin{lstlisting}[language=bash]
./app.sh
\end{lstlisting}

\subsubsection{Create and Add security group for CouchDB }
Execute following to create a security group that opens internal ports 4369, 5000, 5986, 6379, 9100-9200
\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts.yaml db_security.yaml 
\end{lstlisting}

\subsubsection{Create CouchDB instances }
Install CouchDB under couchDB group using docker-compose and bind to external volume. Tag is used to skip killing container task
\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts_auto.ini applications.yaml
#--skip-tags "kill_container"
\end{lstlisting}

\subsubsection{Create CouchDB Cluster }
Cluster 3 instances CouchDB
\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts_auto.ini couchDB_setup.yaml  
\end{lstlisting}

\subsubsection{Create CouchDB Views}
This command creates and update views periodically 
\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts_auto.ini couchdb_view.yaml 
\end{lstlisting}

\subsubsection{Harvester}
Harvester can be deployed under any instances using following playbook by just changing the variable.
\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts_auto.ini harvest.yaml 
\end{lstlisting}

\subsubsection{Web App}
Both geo and web app applications are install through this playbook
\begin{lstlisting}[language=bash]
ansible-playbook -i inventories/hosts_auto.ini webapp.yaml 
\end{lstlisting}

%\subsection{Viewing results}
%Once the application is running, run the following command under ~/geo to run the web application.
%\begin{lstlisting}[language=bash]
%python3 web.py
%\end{lstlisting}
