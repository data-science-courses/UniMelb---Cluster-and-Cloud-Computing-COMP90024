# -*- coding: utf-8 -*-
"""prototype.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NQAi_u_S9HQudSjBeei2DtE9tQnoRPaD

# Set up
[MPI for Python](https://mpi4py.readthedocs.io/en/stable/install.html#using-pip-or-easy-install)
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install mpi4py # MPI for Python

from google.colab import drive
import os
import json
from collections import Counter
import re
from pprint import pprint

# Mount Drive before changing working directory
drive.mount('/content/drive')
# %cd "/content/drive/My Drive/unimelb-cluster-and-cloud-computing-comp90024-2020-sm1/twitter_hashtags"

"""# Data

Each line of data file (except first line/last line) contains json text representing
[Tweet Data Dictionary](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object), under field "doc".
"""

datafile = "data/tinyTwitter.json"
with open(datafile) as f:
  lines = f.readlines()

print(lines[0]) # 1st line
print(lines[1]) # 2nd line
print(lines[-1]) # last line

"""# Process line (elementary workflow)

## Remove unwanted characters

As each line may end with characters (e.g ',' ']' '}') resulting in badly formatted json text, those trailing characters need to be removed.
"""

def remove_trails(text):
  """
  Remove unwanted trailing characters from json text
  text: line of json text
  """

  text = text.rstrip() # remove trailing whitespace (\n)
  text = re.sub(r"(?<=}),?]?}?$", "", text) # remove unwanted trailing ',' ']' '}'

  return text

# Test
print(remove_trails('{"id":"1212","doc":{"_id":"1212"}},\n'))
assert remove_trails('{"id":"1212","doc":{"_id":"1212"}},\n') == '{"id":"1212","doc":{"_id":"1212"}}'
assert remove_trails('{"id":"1212","doc":{"_id":"1212"}}]}\n') == '{"id":"1212","doc":{"_id":"1212"}}'

# Real data
lines_clean = list(map(remove_trails, lines))
print(*lines_clean[:3], sep="\n")

"""## Parse for Tweet Data Dictionary"""

def parse_tweet(text):
  """
  Parse for Tweet Data Dictionary in json text, under field "doc"
  text: line of json text
  """

  try:
    data = json.loads(text)
    tweet = data["doc"]
  except json.decoder.JSONDecodeError: # illegal text
    tweet = {} 

  return tweet

# Test
print(parse_tweet('{"id":"1212","doc":{"_id":"1212"}}'))
assert parse_tweet('{"id":"1212","doc":{"_id":"1212"}}') == {"_id":"1212"}
assert not parse_tweet('{"total_rows":215443567,"offset":211386044,"rows":[') # not any tweet data

# Real data
tweets = [t for t in map(parse_tweet, lines_clean) if t] # skip first line
pprint(tweets[-1])

"""## Search for Hashtags

Array of [Hashtag Objects](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/entities-object#hashtags) is under [Entities data dictionary](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/entities-object#entitiesobject)
"""

def extract_hashtags(tweet):
  """
  Extract hashtags from Tweet Data Dictionary and convert to lowercase
  tweet: Tweet Data Dictionary
  """

  if not tweet:
    return []

  hashtags = tweet["entities"]["hashtags"]
  names = [tag["text"].lower() for tag in hashtags] # lowercased hashtag names (without #)

  return names

htcounts = Counter()
for t in tweets:
  names = extract_hashtags(t)
  htcounts.update(names)
print(htcounts)

htcounts.most_common(10)

"""## Search for language in a tweet

Language code is under field 'lang'.  
[Language names](https://developer.twitter.com/en/docs/twitter-for-websites/twitter-for-websites-supported-languages/overview) supported by Twitter will be used to make report.
"""

langnames = {'ar': 'Arabic',
             'bn': 'Bengali',
             'cs': 'Czech',
             'da': 'Danish',
             'de': 'German',
             'el': 'Greek',
             'en': 'English',
             'es': 'Spanish',
             'fa': 'Persian',
             'fi': 'Finnish',
             'fil': 'Filipino',
             'fr': 'French',
             'he': 'Hebrew',
             'hi': 'Hindi',
             'hu': 'Hungarian',
             'id': 'Indonesian',
             'it': 'Italian',
             'ja': 'Japanese',
             'ko': 'Korean',
             'msa': 'Malay',
             'nl': 'Dutch',
             'no': 'Norwegian',
             'pl': 'Polish',
             'pt': 'Portuguese',
             'ro': 'Romanian',
             'ru': 'Russian',
             'sv': 'Swedish',
             'th': 'Thai',
             'tr': 'Turkish',
             'uk': 'Ukrainian',
             'ur': 'Urdu',
             'vi': 'Vietnamese',
             'zh-cn': 'Chinese Simplified',
             'zh-tw': 'Chinese Traditional'}

def extract_language(tweet):
  """
  Extract language from Tweet Data Dictionary
  """
  
  return tweet.get("lang", "")

langcounts = Counter(map(extract_language, tweets))
print(langcounts)

pprint(langcounts.most_common(10))

"""## Tweet Processor"""

class Tweet:
    """Process line of json text for single tweet data"""

    def __init__(self, text):
      """
      text: line of json text
      """

      self.text = text

      # Tweet data
      text_clean = remove_trails(text)
      self.data = parse_tweet(text_clean)

      # Extract information
      self.hashtags = extract_hashtags(self.data)
      self.lang = extract_language(self.data) # language

tweets_pro = [t for t in map(Tweet, lines) if t.data] # drop badly formatted lines

# Count hashtags, languages
htcounts_pro = Counter()
langcounts_pro = Counter()
for t in tweets_pro:
  htcounts_pro.update(t.hashtags)
  langcounts_pro[t.lang] += 1
print(htcounts_pro)
print(langcounts_pro)

"""# Program (process big file)

## Read specific chunk
"""

def read_lines(filename, start=0, end=-1):
  """
  Read specific chunk of filename line by line (lazy)
  filename: json file containing tweet data (big file)
  start: byte position to read from (defaults to start of file)
  end: byte position to read to (defaults to end of file)
  """

  if end < 0:
    end = os.path.getsize(filename)

  with open(filename) as f:
    f.seek(start)
    while f.tell() < end:
      yield f.readline()

# Test
filename = '/tmp/data.txt'
with open(filename, 'w') as fh:
  fh.write("Hello World!\nHow are you today?\nThank you!")
print(*read_lines(filename, end=15), sep="\n")
print(*read_lines(filename, start=15, end=20), sep="\n")
print(*read_lines(filename, start=20), sep="\n")

# Real data
print(*list(read_lines(datafile))[:3], sep="\n") # first 3 lines

"""## Count hashtags and languages in chunk"""

def count_hashtags_langs(filename, start=0, end=-1):
  """
  Count hashtags and languages in specific chunk of filename
  filename: json file containing tweet data (big file)
  start: byte position to read from (defaults to start of file)
  end: byte position to read to (defaults to end of file)
  """
  
  htcounts = Counter() # hashtag counts
  langcounts = Counter() # language counts
  for text in read_lines(filename, start, end):
    tweet = Tweet(text)
    if not tweet.data: # badly formatted line
      continue
    
    htcounts.update(tweet.hashtags)
    langcounts[tweet.lang] += 1

  return htcounts, langcounts

# Non-splitting
htcounts, langcounts = count_hashtags_langs(datafile)
print(htcounts)
print(langcounts)

# Splitting
htcounts1, langcounts1 = count_hashtags_langs(datafile, end=1567912)
htcounts2, langcounts2 = count_hashtags_langs(datafile, start=1567912, end=3135824)
htcounts3, langcounts3 = count_hashtags_langs(datafile, start=3135824)
htcounts_split = htcounts1 + htcounts2 + htcounts3
langcounts_split = langcounts1 + langcounts2 + langcounts3
print(htcounts_split)
print(langcounts_split)

"""## Process chunk"""

def process_chunk(filename, chunks, number):
  """
  Pick and process equal-size chunk of filename according to its number
  filename: json file containing tweet data (big file)
  chunks: total number of chunks
  number (starting 0): current chunk number
  """

  size = os.path.getsize(datafile)
  start = int(number / chunks * size) # start byte position
  end = int((number+1) / chunks * size) # end byte position
  htcounts, langcounts = count_hashtags_langs(filename, start, end)

  return htcounts, langcounts

# Split chunks
htcounts1, langcounts1 = process_chunk(datafile, 3, 0)
htcounts2, langcounts2 = process_chunk(datafile, 3, 1)
htcounts3, langcounts3 = process_chunk(datafile, 3, 2)
htcounts_split = htcounts1 + htcounts2 + htcounts3
langcounts_split = langcounts1 + langcounts2 + langcounts3
print(htcounts_split)
print(langcounts_split)

"""## Sum respective counts from all chunks and find overall top 10s"""

N = 3 # number of chunks

def main():
  htcounts_all = Counter() # overall hashtag counts
  langcounts_all = Counter() # overall language counts
  for i in range(N): # for each chunk number
    htcounts, langcounts = process_chunk(datafile, N, i)
    htcounts_all += htcounts
    langcounts_all += langcounts
  print(htcounts_all)
  print(langcounts_all)
  pprint(htcounts_all.most_common(10))
  pprint(langcounts_all.most_common(10))

main()

import regex

text = "can't, Å, é, and #中ABC _ #sh_t! #abc #มาพ่องเพิ่งอะไร"
print(regex.findall('#\w+', text))

"""# Run application"""

# Commented out IPython magic to ensure Python compatibility.
# %run -i "__main__.py"

