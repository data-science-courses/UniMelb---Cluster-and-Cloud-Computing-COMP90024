\documentclass[11pt]{article}
\usepackage{colacl}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{url}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}
\usepackage{pgfplots}
\pgfplotsset{width=6.5cm,compat=2.0}
\sloppy

\title{Cluster and Cloud Computing - HPC Twitter Processing}
\author{Adam Mooy\\
	    University of Melbourne\\
	    {\tt hmooy@student.unimelb.edu.au}
  \And Vinh Nguyen\\
  	University of Melbourne\\
	    {\tt vinh@student.unimelb.edu.au}}


\begin{document}
\maketitle

\section{Objective}

The goal of this assignment is to implement a simple, parallelise application leveraging the University of Melbourne HPC facility SPARTAN. The application will be used on a large Twitter data set to identify the top 10 most commonly used hashtags and most commonly used languages. 

\section{Twitter Data Set}
The big twitter dataset comes in JSON format and has 4057522 rows of tweets from Sydney with a size of 19.34 GB. Every Tweet object\footnote{a.k.a Tweet Data Dictionary} includes an \textit{entities} section and a \textit{lang} section which provide metadata such as hashtags and language respectively. We decided to extract and count this readily available metadata. 

\section{Application details}
\subsection{Single process operation}
Every process is committed to a unique equal-size chunk of the large data file based on its rank. This assignment is achieved by instructing the process to compute the start and end byte positions for itself, following these equations
\[\textrm{start} =  \frac{\textrm{rank} \times \textrm{filesize}}{\textrm{number of processes}} \] 
\[\textrm{end} =  \frac{(\textrm{rank}+1) \times \textrm{filesize}}{\textrm{number of processes}} \] 

All processes read their assigned chunks line by line. Every input line is first removed of any unwanted trailing characters like ',' ']' before being parsed into a dictionary that contains the information of our interest. Specifically, the hashtag texts and the name of the language used in a tweet can be found by looking into the fields \textit{hashtags} and \textit{lang}, in the same order. Each process then updates its counts for each hashtag and language accordingly. 

\subsection{Parallel processing}
The single process design allows for a high level of independence between the parallel processes. In fact, there is no need for communication between the processes until they have all finalized their counts for each hashtag and language. Only then these counts are sent to Rank 0, where they are added together with their counterparts, to reveal the top 10 most common hashtags and languages.

\subsection{Invocation}

For job submission, the user needs to run the following shell command:
\begin{flushleft}
\t sbatch --nodes=\$nodes --ntasks=\$ntasks --time=\$time --export=ALL,DATAFILE=\$file\\ --output=results/\$\{nodes\}n\$\{ntasks\}c-\\\$\{file\#\#*/\}-\%j.out \$script
\end{flushleft}
where: \$nodes = number of nodes (e.g 1), \$ntasks = number of tasks (e.g 8), \$time = time limit for job (dependent), \$file = path to data file (data/bigTwitter.json), \$script = path to job script (job.slurm).

Inside the slurm script, our application is invoked via the \t srun command. Output files are stored in folder 'results'.

\section{Experiments}

Theoretically, Cloud nodes are comparatively slower, especially on multiple node jobs because the communication between Cloud nodes is slower than Physical nodes. In contrast, Physical nodes are connected by high-speed 25Gb networking with 1.15 µsec latency which is preferable for multi-node jobs. When experimenting on small and tiny twitter files, Cloud partition execution time is faster than Physical partition. The reason could be the traffic wasn't that bad; therefore, it has lower latency and outperforms Physical nodes.  However, since both small and tiny Twitter file sizes are relatively small compared to big Twitter, Physical partition is used when running application on the big Twitter file. In order to have a proper comparison, Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz nodes were used.

We run our application in various modes in order to evaluate changes in performance (runtime) when there is a change in number of tweets processed and computing resources (nodes and cores).

\begin{table}[h]
 \begin{center}
\begin{tabular}{|l|l|l|l|l|}

      \hline
       &1N1C & 1N8C & 2N8C  \\
      \hline\hline
      999 & 0.586s & 0.734s & 0.823s \\
      4999 & 1.092s & 2.1s & 1.4s \\
      4057522 & 12m12s & 1m38s & 1m36.6s \\

     \hline

\end{tabular}
\caption{Runtime on different resources across different amounts of tweets}\label{table2}
 \end{center}
\end{table}


\begin{tikzpicture}
\begin{axis}[
    ybar,
    enlargelimits=0.15,
    legend style={at={(0.5,-0.15)},
      anchor=north,legend columns=-1},
    ylabel={Time(s)},
    symbolic x coords={1N1C,1N8C,2N8C},
    xtick=data,
    nodes near coords,
    nodes near coords align={vertical},
    ]
\addplot coordinates {(1N1C,817.306) (1N8C,103.585) (2N8C,105.09)};
\end{axis}
\end{tikzpicture}

These results are obtained from attached 1n1c-bigTwitter.json-15909060.out 1n8c-bigTwitter.json-15909058.out and 2n8c-bigTwitter.json-15909059.out slurm outputs.

Regardless of what resources we use, we 
\begin{table}[h]
 \begin{center}
\begin{tabular}{|l|l|l|l|l|}

      \hline
      Rank &Hashtags & Counts   \\
      \hline\hline
      1 & auspol & 19878 \\
      2 & coronavirus & 10110 \\
      3 & มาพ่องเพิ่งอะไระไร & 7531 \\
      4 & firefightaustralia & 6812 \\
      5 & oldme & 6418 \\
      6 & sydney & 6196 \\
      7 & scottyfrommarketing & 5185\\
      8 & grammys & 5085 \\
      9 & assange & 4689 \\
      10 & sportsrorts & 4516 \\
    
     \hline

\end{tabular}
\caption{Top 10 most common hashtags}\label{table1}
 \end{center}
\end{table}


\begin{table}[h]
 \begin{center}
\begin{tabular}{|l|l|l|l|l|}

      \hline
      Rank &Languages & Counts   \\
      \hline\hline
      1 & English (en) & 3107115 \\
      2 & Undefined (un) & 252117 \\
      3 & Thai (th) & 134571 \\
      4 & Portuguese (pt) & 125858 \\
      5 & Spanish (es) & 74028 \\
      6 & Japanese (ja) & 49929 \\
      7 & Tagalog (tl) & 44560\\
      8 & Indonesian (in) & 42296 \\
      9 & French (fr) & 38098 \\
      10 & Arabic (ar) & 24501 \\
    
     \hline

\end{tabular}
\caption{Top 10 most common languages}\label{table2}
 \end{center}
\end{table}


\begin{table}[h]
 \begin{center}
\begin{tabular}{|l|l|l|l|l|}

      \hline
      Rank &1N1C & 1N8C & 2N8C  \\
      \hline\hline
      0 & 816.78 & 100.43 & 102.29 \\
      1 & - & 100.32 & 102.18 \\
      2 & - & 100.32 & 102.18 \\
      3 & - & 100.32 & 102.18 \\
      4 & - & 100.32 & 102.18 \\
      5 & - & 100.32 & 102.18 \\
      6 & - & 100.32 & 102.18 \\
      7 & - & 100.32 & 102.18 \\

     \hline

\end{tabular}
\caption{Top 10 most common languages}\label{table2}
 \end{center}
\end{table}

\section{Discussion}

Figure 1 shows the time spent to run the application under different resources. As expected the amount of time to run the application using 8 cores would be quicker than single-core. 2 Nodes 8 Cores is slightly slower than 1 Node 8 Cores because there is a communication time between 2 Nodes. 

From Table 1, execution time on each cores were recorded to ensure tasks were evenly distributed among all processes. Rank 0 took slightly longer as it has to gather all the counts from each processes and computed the total count. 

 


\section{Conclusion}



\section{INCLUDE OR NOT}



\bibliography{references}


\end{document}
